{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the person's name (or type 'done' to finish):  Ali\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images for Ali. Press 'q' to stop.\n",
      "Captured 1/100 images for Ali\n",
      "Captured 2/100 images for Ali\n",
      "Captured 3/100 images for Ali\n",
      "Captured 4/100 images for Ali\n",
      "Captured 5/100 images for Ali\n",
      "Captured 6/100 images for Ali\n",
      "Captured 7/100 images for Ali\n",
      "Captured 8/100 images for Ali\n",
      "Captured 9/100 images for Ali\n",
      "Captured 10/100 images for Ali\n",
      "Captured 11/100 images for Ali\n",
      "Captured 12/100 images for Ali\n",
      "Captured 13/100 images for Ali\n",
      "Captured 14/100 images for Ali\n",
      "Captured 15/100 images for Ali\n",
      "Captured 16/100 images for Ali\n",
      "Captured 17/100 images for Ali\n",
      "Captured 18/100 images for Ali\n",
      "Captured 19/100 images for Ali\n",
      "Captured 20/100 images for Ali\n",
      "Captured 21/100 images for Ali\n",
      "Captured 22/100 images for Ali\n",
      "Captured 23/100 images for Ali\n",
      "Captured 24/100 images for Ali\n",
      "Captured 25/100 images for Ali\n",
      "Captured 26/100 images for Ali\n",
      "Captured 27/100 images for Ali\n",
      "Captured 28/100 images for Ali\n",
      "Captured 29/100 images for Ali\n",
      "Captured 30/100 images for Ali\n",
      "Captured 31/100 images for Ali\n",
      "Captured 32/100 images for Ali\n",
      "Captured 33/100 images for Ali\n",
      "Captured 34/100 images for Ali\n",
      "Captured 35/100 images for Ali\n",
      "Captured 36/100 images for Ali\n",
      "Captured 37/100 images for Ali\n",
      "Captured 38/100 images for Ali\n",
      "Captured 39/100 images for Ali\n",
      "Captured 40/100 images for Ali\n",
      "Captured 41/100 images for Ali\n",
      "Captured 42/100 images for Ali\n",
      "Captured 43/100 images for Ali\n",
      "Captured 44/100 images for Ali\n",
      "Captured 45/100 images for Ali\n",
      "Captured 46/100 images for Ali\n",
      "Captured 47/100 images for Ali\n",
      "Captured 48/100 images for Ali\n",
      "Captured 49/100 images for Ali\n",
      "Captured 50/100 images for Ali\n",
      "Captured 51/100 images for Ali\n",
      "Captured 52/100 images for Ali\n",
      "Captured 53/100 images for Ali\n",
      "Captured 54/100 images for Ali\n",
      "Captured 55/100 images for Ali\n",
      "Captured 56/100 images for Ali\n",
      "Captured 57/100 images for Ali\n",
      "Captured 58/100 images for Ali\n",
      "Captured 59/100 images for Ali\n",
      "Captured 60/100 images for Ali\n",
      "Captured 61/100 images for Ali\n",
      "Captured 62/100 images for Ali\n",
      "Captured 63/100 images for Ali\n",
      "Captured 64/100 images for Ali\n",
      "Captured 65/100 images for Ali\n",
      "Captured 66/100 images for Ali\n",
      "Captured 67/100 images for Ali\n",
      "Captured 68/100 images for Ali\n",
      "Captured 69/100 images for Ali\n",
      "Captured 70/100 images for Ali\n",
      "Captured 71/100 images for Ali\n",
      "Captured 72/100 images for Ali\n",
      "Captured 73/100 images for Ali\n",
      "Captured 74/100 images for Ali\n",
      "Captured 75/100 images for Ali\n",
      "Captured 76/100 images for Ali\n",
      "Captured 77/100 images for Ali\n",
      "Captured 78/100 images for Ali\n",
      "Captured 79/100 images for Ali\n",
      "Captured 80/100 images for Ali\n",
      "Captured 81/100 images for Ali\n",
      "Captured 82/100 images for Ali\n",
      "Captured 83/100 images for Ali\n",
      "Captured 84/100 images for Ali\n",
      "Captured 85/100 images for Ali\n",
      "Captured 86/100 images for Ali\n",
      "Captured 87/100 images for Ali\n",
      "Captured 88/100 images for Ali\n",
      "Captured 89/100 images for Ali\n",
      "Captured 90/100 images for Ali\n",
      "Captured 91/100 images for Ali\n",
      "Captured 92/100 images for Ali\n",
      "Captured 93/100 images for Ali\n",
      "Captured 94/100 images for Ali\n",
      "Captured 95/100 images for Ali\n",
      "Captured 96/100 images for Ali\n",
      "Captured 97/100 images for Ali\n",
      "Captured 98/100 images for Ali\n",
      "Captured 99/100 images for Ali\n",
      "Captured 100/100 images for Ali\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the person's name (or type 'done' to finish):  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset collection complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "DATASET_DIR = './dataset'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    person_name = input(\"Enter the person's name (or type 'done' to finish): \").strip()\n",
    "    if person_name.lower() == 'done':\n",
    "        print(\"Dataset collection complete!\")\n",
    "        break\n",
    "\n",
    "    person_dir = os.path.join(DATASET_DIR, person_name)\n",
    "    if os.path.exists(person_dir):\n",
    "        print(f\"Images for {person_name} already exist. Skipping...\")\n",
    "        continue\n",
    "    os.makedirs(person_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Collecting images for {person_name}. Press 'q' to stop.\")\n",
    "    count = 0\n",
    "\n",
    "    while count < 100:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        # Convert to grayscale for better face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            padding = 10\n",
    "            x1, y1 = max(0, x - padding), max(0, y - padding)\n",
    "            x2, y2 = min(frame.shape[1], x + w + padding), min(frame.shape[0], y + h + padding)\n",
    "\n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Save the cropped face\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "            img_path = os.path.join(person_dir, f\"{person_name}_{count}.jpg\")\n",
    "            cv2.imwrite(img_path, face_roi)\n",
    "            count += 1\n",
    "            print(f\"Captured {count}/100 images for {person_name}\")\n",
    "\n",
    "            if count >= 100:\n",
    "                break\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Face Capture - Press 'q' to Quit\", frame)\n",
    "\n",
    "        # Quit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessed successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "IMG_SIZE = 128  # Image size for input\n",
    "DATA_DIR = './dataset'\n",
    "\n",
    "# Get class labels from dataset folder names\n",
    "persons = [name for name in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, name))]\n",
    "labels = {name: i for i, name in enumerate(persons)}\n",
    "\n",
    "# Load dataset images\n",
    "X, y = [], []\n",
    "for person in persons:\n",
    "    folder = os.path.join(DATA_DIR, person)\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                X.append(img)\n",
    "                y.append(labels[person])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "\n",
    "X = np.array(X) / 255.0  # Normalize images\n",
    "y = np.array(y)\n",
    "\n",
    "if len(X) == 0:\n",
    "    print(\"No valid data found. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Split dataset into training & validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = to_categorical(y_train, num_classes=len(persons))\n",
    "y_val = to_categorical(y_val, num_classes=len(persons))\n",
    "\n",
    "print(\"Dataset preprocessed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 264ms/step - accuracy: 0.5052 - loss: 0.7422 - val_accuracy: 0.4750 - val_loss: 0.6728\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.6431 - loss: 0.6528 - val_accuracy: 0.8750 - val_loss: 0.6321\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.6350 - loss: 0.6176 - val_accuracy: 0.8250 - val_loss: 0.6019\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.6684 - loss: 0.5669 - val_accuracy: 0.6000 - val_loss: 0.5930\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.6964 - loss: 0.5555 - val_accuracy: 0.8250 - val_loss: 0.5625\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step - accuracy: 0.7627 - loss: 0.5542 - val_accuracy: 0.8750 - val_loss: 0.5324\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step - accuracy: 0.7943 - loss: 0.5141 - val_accuracy: 0.8750 - val_loss: 0.5113\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 154ms/step - accuracy: 0.8687 - loss: 0.4699 - val_accuracy: 0.8750 - val_loss: 0.4878\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.8509 - loss: 0.4330 - val_accuracy: 0.8750 - val_loss: 0.4676\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 157ms/step - accuracy: 0.8686 - loss: 0.4134 - val_accuracy: 0.8750 - val_loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the preprocessed data\n",
    "IMG_SIZE = 128  # Image size used in preprocessing\n",
    "NUM_CLASSES = len(persons)  # Number of classes (persons)\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=16)\n",
    "\n",
    "# Save the model\n",
    "model.save('attendance_resnet_model.h5')\n",
    "print(\"ResNet50 model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'attendance_transfer_learning_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattendance_transfer_learning_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load Haar Cascade for face detection\u001b[39;00m\n\u001b[0;32m      9\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\alleys\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\alleys\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\alleys\\lib\\site-packages\\h5py\\_hl\\files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\alleys\\lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'attendance_transfer_learning_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model\n",
    "model = load_model('attendance_resnet_model.h5')\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Define image size\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Preprocess face for model\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            face_roi = cv2.resize(face_roi, (IMG_SIZE, IMG_SIZE))\n",
    "            face_roi = np.expand_dims(face_roi, axis=0) / 255.0\n",
    "\n",
    "            # Predict person\n",
    "            predictions = model.predict(face_roi)\n",
    "            person_index = np.argmax(predictions)\n",
    "            confidence = np.max(predictions)\n",
    "\n",
    "            label = list(labels.keys())[person_index] if confidence > 0.7 else \"Unidentified\"\n",
    "\n",
    "            # Display result\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition - Press 'Q' to Quit\", frame)\n",
    "\n",
    "        # Check for quit command\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Press 'q' to exit\n",
    "            print(\"\\n[INFO] Quitting application...\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[INFO] Keyboard Interrupt detected. Exiting gracefully...\")\n",
    "\n",
    "finally:\n",
    "    # Release resources and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"[INFO] Resources released. Application closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
